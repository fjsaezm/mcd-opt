\documentclass[11pt,table]{article}

    \usepackage[table]{xcolor}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    

    %\usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}

  
    \newcommand{\qed}{\hfill $\square$}

    \newtheorem{nth}{Theorem}
    

    % Document title
    \title{Convex Optimization}

    
\usepackage{mdframed}


\newenvironment{problem}[2][Exercise]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}

% Define solution environment
\newenvironment{solution}
    {\textit{Solución}}
    {}
    


    \usepackage{fancyhdr} % Headers and footers
    \pagestyle{fancy} % All pages have headers and footers
    \fancyhead{}\renewcommand{\headrulewidth}{0pt} % Blank out the default header
    \fancyfoot[L]{} % Custom footer text
    \fancyfoot[C]{} % Custom footer text
    \fancyfoot[R]{\thepage} % Custom footer text
    \newcommand{\note}[1]{\marginpar{\scriptsize \textcolor{red}{#1}}} % Enables comments in red on margin
    
    


    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    \definecolor{linkcolor}{HTML}{303F9F}

    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    
    \pagestyle{fancy}

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
% COMMANDS

\newcommand\norm[1]{\lVert#1\rVert}
\newcommand\abs[1]{\lvert#1\rvert}
\newcommand\R{\mathbb R}
    

\begin{document}

%-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth}
  \raggedright
  \footnotesize
  Francisco Javier Sáez Maldonado \hfill\\
  José Antonio Álvarez Ocete \hfill\\
\end{minipage}
\begin{minipage}{0.4\textwidth}
  \centering
  \large
  Ejercicios Programación Lineal\\
  \normalsize
  Optimización\\
\end{minipage}
\begin{minipage}{0.295\textwidth}
  \raggedleft
  \today\hfill\\
\end{minipage}
\medskip\hrule
\bigskip

%-------------------------------
%	CONTENTS
% -------------------------------

\begin{problem}{1}
Show that if \( S \) is an open set, its complement \( S^c \) is closed, and viceversa.
\end{problem}

\(\boxed{\Rightarrow}\)\\

Let \(S\) be an open set. We know that \(S^c\) is a closed set if and only if for any sequence of elements \(\{x_n\} \subset S^c\) such that \(\{x_n\} \longrightarrow x\), then \(x \in S^c\). \\

Let us use this characterization to prove that \(S^c\) is closed. Let \(\{x_n\} \subset S^c\) such that \(\{x_n\} \longrightarrow x\). Suppose that \(x\) is not in \(S^c\). Then \(x\) must be in its complementary, \(S\). Since \(S\) is an open set we know that \(\exists \epsilon > 0\) such that \( B(x, \epsilon) \subset S\). \\

Since \(\{x_n\} \longrightarrow x\), for any \(\delta > 0 \exists n \in \mathbb{N}\) such that \( \parallel x - x_n \parallel < \delta\). In particular, for \(\delta = \epsilon\) there is a element of the succesion \(x_n\) in \( B(x, \epsilon) \subset S\), but \(\{x_n\} \subset S^c\). This contradiction implies that \(x\) is, in fact, in \(S^c\).

\(\boxed{\Leftarrow}\)\\

Let \(S\) be a set such that its complement $S^c$ is closed (that is, \(S^c = cl(S^c)\)). Let us show that \(S\) is open. \\

Let \(x \in S\). Then, \(x \notin S^c\), which implies \(x \notin cl(S^c)\). This implies that

\[
  \exists \epsilon > 0 \text{ such that } B(x,\epsilon) \cap S^c = \emptyset \implies B(x,\epsilon) \subset S
\]

Thus, $S$ is open. \qed \\

\begin{problem}{2}
If \( S_1, S_2 \) are convex subsets, prove that the following are also convex sets:
\begin{align*}
  S_{1} \cap S_{2} & = \{x \ : \ x \in S_{1} \text{ and } x \in S_{2}\} \\
  S_{1} + S_{2}    & = \{x + x' \ : \ x \in S_{1}, x' \in S_{2}\}       \\
  S_{1} - S_{2}    & = \{x - x' \ : \ x \in S_{1}, x' \in S_{2}\}
\end{align*}

\end{problem}

\begin{itemize}
  \item Consider \(S_{1} \cap S_{2}  =  \{x \ : \ x \in S_{1} \text{ and } x \in S_{2}\}\). Let \(x,x' \in S_1 \cap S_2\) and consider the segment
        \[
          z \equiv \lambda x + (1-\lambda)x', \lambda \in [0,1]
        \]

        Since \(x\) is in in \(S_1\), which is a convex set, \(z\) will also be in \(S_1\) for any \(\lambda \in [0,1]\). Likewise, \(z\) will be in \(S_2\), and thus in the intersection \(S_{1} \cap S_{2}\). Since the segment is contained in the intersection, \(S_{1} \cap S_{2}\) is a convex subset.

  \item Consider \(S_{1} + S_{2}  =  \{x + z \ : \ x \in S_{1}, z \in S_{2}\}\). Now, let \(x+z \in S_1 + S_2\) and \(x'+z' \in S_1+S_2\). Also, consider the segment
        \begin{align*}
          \lambda(x+z) + (1-\lambda)(x'+z') & = \lambda x + \lambda z + (1-\lambda)x' + (1-\lambda z')                                                     \\
                                            & = \underbrace{\lambda x +(1-\lambda)x'}_{x'' \in S_1} + \underbrace{\lambda z + (1-\lambda)z'}_{z'' \in S_2}
        \end{align*}
        Where the underbraces are true due to the convexity of \(S_1\) and \(S_2\), so we have
        \[
          x'' + z'' \in S_1 + S_2
        \]
        so the segment is in the sum set, and thus, the set \(S_1 + S_2\) is convex.

  \item We use the same process done in the previous set. Consider \(S_{1} - S_{2}  =  \{x - z \ : \ x \in S_{1}, z \in S_{2}\}\). Now, let \(x-z \in S_1 - S_2\) and \(x'-z' \in S_1-S_2\). Also, consider the segment
        \begin{align*}
          \lambda(x-z) + (1-\lambda)(x'-z') & = \lambda x - \lambda z + (1-\lambda)x' - (1-\lambda z')                                                                   \\
                                            & = \underbrace{\lambda x +(1-\lambda)x'}_{x'' \in S_1} - \underbrace{\left( \lambda z + (1-\lambda)z'\right)}_{z'' \in S_2}
        \end{align*}
        Where the underbraces are true due to the convexity of \(S_1\) and \(S_2\), so we have
        \[
          x'' - z'' \in S_1 - S_2
        \]
        so the segment is in the difference set, and thus, the set \(S_1 - S_2\) is convex.
\end{itemize}

\begin{problem}{3}
If \( f: S \to \mathbb{R} \) is a convex function on the convex set \( S \), the set \( S_{min} = \{x : \ x \text{ is a minimum of f} \} \) is a convex set.
\end{problem}

We omit the case where \(S_{min} = \emptyset\), since the empty set is convex. Now, let \(y\) be the minimum of \(f(x)\): \(y = \min_x f(x)\). Then \( S_{min} = \{x \in S : \ f(x) = y \} \). We need need to show that for all \(x, x' \in S_{min}\) and for all \(\lambda \in [0,1]\):

\[
  z \equiv \lambda x + (1-\lambda) x' \in S_{min} \leftrightarrow f(z) = y
\]

Since \(S\) is convex, \(z\) is in \(S\), and since \(f\) is also convex:
\begin{align*}
  f(z) & = f\left(\lambda x + (1-\lambda) x'\right) \\
       & \leq \lambda f(x) + (1-\lambda) f(x')      \\
       & = \lambda y + (1-\lambda) y                \\
       & = y                                        \\
\end{align*}
where we used that \(x, x'\in S_{min}\). But since \(y\) is the minimum of \(f\), the equality holds \(f(z) = y\). This means that \(z\) is in \(S_{min}\), therefore \(S_{min}\) is a convex set.

\begin{problem}{4}
Given a quadratic form \( q(w) = w^TQw + bw + c \), with \( Q \) a symmetric \( d\times d \) matrix, \( w,b\) being \(d\times 1 \) vectors and \( c \) a real number, derive its gradient and Hessian.
\[
  \nabla q(w) = Qw + b, \quad Hq(w) = Q
\]
\emph{Hint: expand \(q(w)\) and take the partials with respect to \(w_i\) and \(w_i,w_j\).}
\end{problem}

Let us start by unrolling the quadratic form expression:

\[
  q(w) = \sum_{i,j=1}^{d} Q_{ij} w_{i}w_{j} + \sum_{i=1}^{d}b_{i}w_{i} + c,
\]

and compute the partial derivative over the \(k-th\) component:

\[
  \frac{\partial q}{\partial w_k} (w) = \sum_{i=1}^{d} Q_{ik} w_{i} + \sum_{j=1}^{d} Q_{kj} w_{j} + b_{k}
\]

where \(k \in \{1, \ldots, d\}\). By using that \(Q\) is symmetric we obtain:

\begin{equation}
  \label{eq-4}
  \frac{\partial q}{\partial w_k} (w) = 2 \; \sum_{j=1}^{d} Q_{kj} w_{j} + b_{k}.
\end{equation}

That is, we are multiplying the \(k-th\) row of the \(Q\) matrix and multiplying it by \(w\). We can obtain gradient as a product of matrices using the previous expression:

\[
  \nabla q(w) = \begin{pmatrix}
    \frac{\partial q}{\partial w_1} (w) \\
    \vdots                              \\
    \frac{\partial q}{\partial w_d} (w)
  \end{pmatrix}
  = \begin{pmatrix}
    2 \; \sum_{j=1}^{d} Q_{1j} w_{j} + b_{1} \\
    \vdots                                   \\
    2 \; \sum_{j=1}^{d} Q_{dj} w_{j} + b_{d}
  \end{pmatrix}
  = 2 \; \begin{pmatrix}
    \sum_{j=1}^{d} Q_{1j} w_{j} \\
    \vdots                      \\
    \sum_{j=1}^{d} Q_{dj} w_{j}
  \end{pmatrix} + \begin{pmatrix}
    b_{1}  \\
    \vdots \\
    b_{d}
  \end{pmatrix}
  = 2 \ Qw + b
\]

In order to obtain the Hessian, we take partial derivatives over the \(l-th\) component in \ref{eq-4}:
\begin{align*}
  \frac{\partial^2 q}{\partial w_k \partial w_l} (w) & = \frac{\partial q}{\partial w_l} \left( \frac{\partial q}{\partial w_k} \right) (w)          \\
                                                     & = \frac{\partial q}{\partial w_l} \left( 2 \; \sum_{j=1}^{d} Q_{kj} w_{j} + b_{k} \right) (w) \\
                                                     & = 2 \; Q_{kl}
\end{align*}
Hence, the Hessian matrix of \(q\) will have \(2 Q_{kl}\) in position \((k, l)\). That is:

\[
  \text{Hess} \ q(w) = 2Q
\]

\begin{problem}{5}
If \( (p_1,\dots,p_K) \) is a probability distribution, prove that its entropy \( H(p_1, \dots, p_K) = - \sum_{i=1}^K p_i \log p_i\) is a concave function. Show also that its maximum is \( \log K \), attained when \( p_i = \frac{1}{K}\) for all $i$.
\end{problem}


In this problem, since we are dealing with probabilities, two new constraints appear:
\begin{align*}
  \sum_{i=1}^K p_i = 1 \\
  p_i \geq 0 \quad \forall i =1,\dots,n
\end{align*}
They will be used later.\\

Let us compute the gradient and Hessian of \(H\) to see that it is concave. Firstly, we have that
\[
  \frac{\partial H}{\partial p_i} = - \log(p_i) - 1, \quad \forall i = 1,\dots,K
\]
and, hence,
\[
  \frac{\partial^2 H}{\partial p_i \partial p_j} = - \frac{\delta{ij}}{p_i},
\]
where $\delta_{ij}$ is \href{https://en.wikipedia.org/wiki/Kronecker_delta}{the Kroneker delta}. Lastly, since \(p_i \geq 0\), we have that the Hessian is a negative-definite diagonal matrix, so \(H\) is concave.\\

In order to find the minimum entropy, we have to solve the following optimization problem:
\begin{align*}
   & \max_{(p_1,\dots,p_K)} H(p_1,\dots,p_K) \\
   & \text{s.t.}                             \\
   & \sum_{i=1}^K p_i -1 = 0                 \\
   & p_i \geq 0 \quad \forall i =1,\dots,K
\end{align*}
Consider the lagrangian of this problem:
\[
  L\left(\{p_i\}_{i=1}^K, \lambda\right) = - \sum_{i=1}^K p_i \log(p_i) + \lambda\left(\sum_{i=1}^K p_i -1\right).
\]
We can obtain its gradient derivating with respect to each variable
\[
  \frac{\partial L}{\partial p_i} = - \log p_i - 1 + \lambda, \quad \frac{\partial L}{\partial \lambda} =  \sum_{i=1}^K p_i -1.
\]
Using this derivatives, we have to equalize them to zero, which is solving the following equations system:
\[
  \begin{cases}
    \log p_i = \lambda - 1, \quad i = 1,\dots,K \\
    \sum_{i=1}^K p_i = 1
  \end{cases}
\]
Looking at the first equation, the \(p_i\) have no interdependencies, they are constant and have the same value. Also, since they have to add \(1\), the only possible solution is that each \(p_i = \frac{1}{K}\). Lastly, we can compute the maximum value of the Entropy:
\[
  H\left(\{p_i\}_{i=1}^K\right) = - \sum_{i=1}^K \frac{1}{K} \log \left(\frac{1}{K}\right)  = - \frac{1}{K} \left(\sum_{i=1}^K \log 1 - \log K\right) = \frac{1}{K} \cdot K \log K = \log K,
\]
as we wanted to prove. \qed

\begin{problem}{6}
We want to solve the following constrained restriction problem:
\begin{align*}
  \min \quad       & x^{2} + 2y^{2} + 4xy \\
  \text{s.t} \quad & x + y = 1            \\
                   & x,y \geq 0.
\end{align*}
\begin{enumerate}
  \item Write its Lagrangian with \(\alpha,\beta\) the multipliers of the inequality constraints.
  \item Write the KKT conditions.
  \item Use them to solve the problem. For this consider separately the \((\alpha = \beta = 0)\), \((\alpha > 0, \beta = 0)\), \((\alpha = 0, \beta > 0)\), \((\alpha > 0, \beta > 0)\) cases.
\end{enumerate}
\end{problem}

Writing the \textbf{Lagrangian} in terms of \(\alpha,\beta,\lambda\) is pretty straightforward:
\[
  L(x,y,\alpha,\beta,\lambda) = x^{2} + 2y^{2} + 4xy + \lambda(x+y - 1) + \alpha x + \beta y .
\]
Now, to write the KKT conditions. As a very brief summarization, the KKT conditions are: the gradient of the Lagrangian equals to zero and the inequality restrictions (multiplied by its corresponding constant) also equal to zero. In our case, the \textbf{KKT conditions} are:
\begin{align*}
  \frac{\partial L}{\partial x}  = 2x + 4y + \lambda + \alpha & = 0 \\
  \frac{\partial L}{\partial y}  = 4y + 4x + \lambda + \beta  & = 0 \\
  \alpha x                                                    & = 0 \\
  \beta y                                                     & = 0
\end{align*}
Now, we want to solve this equations system to see if we can find a minimum of our problem. We have the following cases:

\begin{itemize}
  \item Case \(\alpha = \beta = 0\).\\
        In this case, the system is
        \begin{align*}
          2x + 4y + \lambda & = 0 \\
          4y + 4x + \lambda & = 0 \\
        \end{align*}
        If we substitute \(4y\) from the first equation into the second one, we obtain
        \[
          -2x - \lambda + 4x + \lambda = 0 \implies 2x = 0  \implies x = 0,
        \]
        and, since \(x+y = 1\), we obtain that our \emph{KKT point} is \((0,1)\). Any non-negative values of $\alpha$ and $\beta$ are valid to satisfy both hte KKT conditions and our problem restrictions.
  \item Case \(\alpha, \beta > 0\).\\
        In this case, we obtain from the KKT conditions that \(x=  y = 0\), which does not match our initial conditions \(x+y = 1\), so no \emph{KKT points} are obtained.
  \item Case \(\alpha > 0, \beta = 0\).\\
        Looking at our KKT conditions, since \(\alpha > 0\), we have that \(x = 0\), resulting in \(y=1\) and a \emph{KKT point} \((0,1)\), which is the same that we obtained in the first case.\\
  \item Case \(\alpha = 0, \beta > 0\).\\
        Using the same reasoning, we obtain \((1,0)\) as a new \emph{KKT} point.
\end{itemize}

Until now, we have two candidates to be the optimal one: \(\{(0,1),(1,0)\}\). Now, we make use of the following theorem:

\begin{nth}
  If in a minimization problem with restrictions \(g_i(x), h_j(x) \in C^1\), if we assume \(f\) to be convex and \(h_j\) to be affine, then a KKT point \(x^*\) is an optimum of this problem. (Slide 18)
\end{nth}

So, we can evaluate the function on our KKT points to find the minimum. We obtain that \(f(1,0) = 1, f(0,1) = 2\), so the minimum is reached in \((1,0)\) with optimal value \(1\). \\

\begin{problem}{7}
We have worked out the dual problem for the soft SVC problem. Do the same for the simpler \textbf{hard} SVC problem
\[
  \min_{w,b} \frac{1}{2} \norm{w}^2
\]
subject to \(y^p\left(w \cdot x^p + b\right) \geq 1\). What are here the KKT conditions?
\end{problem}

Firstly, consider the Lagrangian for this problem

\begin{align*}
  L(w,b,\alpha) & = \frac{1}{2} \norm{w}^2 - \sum_p \alpha_p \left[ y^p \left(w \cdot x^p +b\right) -1 \right]       \\
                & = \frac{1}{2} w \cdot w - w \sum_p \alpha_p y^p  x^p - b \sum_p \alpha_p y^p + \sum_p \alpha_p     \\
                & = w \left(\frac{1}{2}w -\sum_p \alpha_p y^p  x^p \right) - b \sum_p \alpha_p y^p + \sum_p \alpha_p
\end{align*}

Then, we have to compute the gradient of the Lagrangian

\[
  \nabla_w L(w,b,\alpha) = w - \sum_p \alpha_p y^p x^p = 0 \implies w = \sum_p \alpha_p y^p x^p
\]
\[
  \frac{\partial L}{\partial b} = - \sum_p \alpha_p y^p = 0
\]

Lastly, we have to use these equalities in the expression of the Lagrangian:

\begin{align*}
  L(w,b,\alpha) & = \sum \alpha_p - \frac{1}{2}\left(\sum_p \alpha_p y^p x^p\right)\left(\sum_q \alpha_q y^q x^q\right) \\
                & = \sum \alpha_p - \frac{1}{2} \sum_{p,q} \alpha_p \alpha_q y^p y^q x^p x^q
\end{align*}

By defining the matrix \(Q\) with value \(y^p y^q x^p x^q\) in position \((p,q)\), ur dual optimization problem gets simplified into

\[
  \begin{cases}
    \max_{\alpha} \sum_p \alpha^p - \frac{1}{2} \alpha^T Q \alpha \\
    \text{s.t.} \alpha^p, \sum \alpha_p y^p = 0
  \end{cases}
\]

At this point we may realize that we have completely removed the dependency \(b\) from our problem. Let us state the KKT conditions for this problem. There are:

\begin{align*}
  \lambda_w f(x^*) + \sum_i \lambda_i \lambda g_i (x^*) & = 0 \\
  \lambda_i g_i(x^*)                                    & = 0
\end{align*}

For our particular problem we obtain:

\begin{align*}
  w + \sum_p \alpha_p y^p x^p & = 0 \\ TODO LA SIGUEITNTE LINEA
  \lambda_i g_i(x^*)          & = 0
\end{align*}

\begin{problem}{8}
A typical Linear Programming (LP) problem can be stated as the following constrained optimization problem:
\begin{align*}
  \min_x c \cdot x \quad s.t. \quad x \geq 0, Ax \leq b
\end{align*}
with \(x \in \R^d\), \(A\) an \(m \times d\) matrix and \(b \in \R^m\). A tool often used in LP is to study the so called dual problem, which in this case is
\begin{align*}
  \min_{z} b \cdot z \quad s.t. \quad z \geq 0, A^t z \leq -c
\end{align*}
with now \(z \in \R^m\). Apply our Lagrangian dual construction technique to show that this is indeed the dual formulation of the initial LP problem
\end{problem}

Firstly, we have to write the Lagrangian for this problem:
\[
  L(x,\lambda,\mu) = c \cdot x -  \sum_{i=1}^d \lambda_i x_i  + \sum_{j=1}^m \mu_j(a_j \cdot x - b_j)
\]
Hence, the gradient respect to \(x\) of the lagrangian is:
\[
  \nabla_x L = c - \lambda + A^t \mu
\]
CHECK!!!\\
(this Lagrangian must have dimension \(d\)).

\begin{problem}{9}
We know that, theoretically, the minimum SVC primal \(f^*\) and the maximum SVC dual \(q^*\) are equal. Check this in this case by writing \(q^*\) and \(f^*\) in terms of the \(\alpha_p^*\) and checking that both expressions coincide.
\end{problem}

\begin{problem}{10}
We want to apply out Lagrangian theory to solve the homogeneous constrained Ridge problem (i.e., with a model \(w\cdot x\)
\[
  \text{arg}\min_{w} \text{ mse}(w) = \frac{1}{n} \sum_{p=1}^n (t^p - w \cdot x^p)^2, \quad \text{s.t.} \quad \norm{w}_2^2 \leq \rho^2.
\]
Write its Lagrangian and, using the lecture slides, the detailed formulation of the KKT conditions at an optimal \(w^*\) and multiplier \(\lambda^*\).\\
Assuming that \(\lambda^* > 0\), use the gradient KKT condition to show that \(w^*\) also solves a standard Ridge regression problem for the optimal value \(\lambda^*\) of the regularization parameter.\\
Assuming now that \(\lambda^* = 0\), use again the slides to write down the solution in this case and use this solution to get a lower bound for \(\rho\).
\end{problem}

HECHO EN CLASE

\begin{problem}{11}
If \( Q \) is a symmetric, positive definite \( d \times d \) matrix, show that \( f(x) = x^TQx, \ x \in \mathbb{R}^d \), is a convex function.
\end{problem}

HECHO EN CLASE

\begin{problem}{12}
Let \( f:\mathbb{R}^d \to \mathbb{R} \)  be a function and assume that \( epi(f) \subset \mathbb{R}^d \times \mathbb{R} \)  is convex. Prove that then \( f \)  is convex.

\end{problem}

Consider the set
\[
  \text{epi}(f) = \{(x,t)\in \R^d \times R \ : \ t \geq f(x)\}.
\]
This set is, by hypothesis, convex. That is, for any \((x,t),(x',t') \in \text{epi}(f)\), we have
\[
  \lambda(x,t) + (1-\lambda)(x',t')  = \left(\lambda x + (1-\lambda)x', \lambda t + (1-\lambda)t'\right) \in \text{epi}(f)\quad \forall \lambda \in [0,1].
\]
This implies that
\begin{equation}\label{epi:1}
  f(\lambda x + (1-\lambda)x') \leq \lambda t + (1-\lambda)t', \quad \forall \lambda \in [0,1].
\end{equation}
Also, since each of the points belongs to \(\text{epi}(f)\), we have that:
\begin{equation}\label{epi:2}
  \lambda f(x) + (1-\lambda)f(x') \leq \lambda t + (1-\lambda)t', \quad \forall \lambda \in [0,1]
\end{equation}
Lastly, if we substract Equation \eqref{epi:2} from Equation \eqref{epi:1} we obtain:
\begin{align*}
   & f(\lambda x + (1-\lambda)x')  - \left( \lambda f(x) + (1-\lambda)f(x')\right)  \leq 0, & \quad \forall \lambda \in [0,1]
  \\ \implies & f(\lambda x + (1-\lambda)x') \leq  \lambda f(x) + (1-\lambda)f(x'), &\quad \forall \lambda \in [0,1].
\end{align*}
Lastly, recalling that \((x,f(x)) \in \text{epi}(f)\) for all \(x \in S\), we obtain that \(f\) is convex. \qed \\


\begin{problem}{13}
Let \( f : \mathbb{R}^d \to \mathbb{R} \) be a convex function. Prove that \( \operatorname{epi}(f) \) is a closed set and that \( (x, f(x)) \in \partial \operatorname{epi}(f) \)  .
\end{problem}



\begin{problem}{14}
Prove that if \( f \)  is strictly convex, it has a unique global minimum.
\end{problem}

HECHO EN CLASE\\

\begin{problem}{15}
Let \( f,g: S \subset \mathbb{R}^d \to \mathbb{R} \)  be two convex functions on the convex set \( S \) . Prove that, as subsets, \(  \partial f(x) + \partial g(x) \subset \partial(f+g)(x) \) for any $x\in S$.
\end{problem}

We already know that \(\xi \in \partial f(x)\) implies that \(f(x') > f(x) + \xi(x-x')\) for all \(x' \in S\). Let us apply this definition to obtain the result.\\
Consider \(\xi_1 \in \partial f(x)\) and \(\xi_2 \in \partial g(x)\). Then, \(\xi_1 + \xi_2 \in \partial f(x) + \partial g(x)\). Now, using the definition for each of the \(\xi_i\) with \(i = 1,2\), we obtain:
\[
  f(x') > f(x) + \xi_1 (x-x'), \quad g(x') > g(x) + \xi_2(x-x')
\]
And, if we add both inequalitys:
\begin{align*}
  f(x') + g(x')        & > f(x) + g(x) + (\xi_1 + \xi_2)(x-x')        \\
  \left(f+g\right)(x') & >\left(f+g\right)(x) + (\xi_1 + \xi_2)(x-x')
\end{align*}
which means that \(\xi_1 + \xi_2 \in \partial \left(f+g\right)(x) \), as we wanted to see. \qed \\

\begin{problem}{16}
Compute the proximal of \( f(x) = 0 \) and of \( g(x) = \frac{1}{2}\|x\|^2 \).
\end{problem}

HECHO EN CLASE\\

\begin{problem}{17}
Assume that \( f \)  is convex. Prove that for any \( \lambda > 0, \partial(\lambda f)(x) = \lambda \partial f(x) \) as subsets.
\end{problem}

HECHO EN CLASE \\

\begin{problem}{18}
Prove that the \(\epsilon-\)insensitive loss function \(\ell_\epsilon(z) = \max\{0,\abs{z - \epsilon}\}\) is convex. Give also its subgradient \(\partial \ell_\epsilon(x)\) at any \(x \in \R\)
\end{problem}



\begin{problem}{19}
Compute the proximals of the hinge \( f(x) = max\{0, -x\}  \) and the \( \epsilon \)-insensitive \( g(x)=max\{0, |x| - \epsilon\} \) loss functions.
\end{problem}

HECHO EN CLASE\\

\begin{problem}{20}
We have seen that we can solve the constrained Ridge problem by a Projected Gradient algorithm. Using the lecture slides, write down in as much detail as you can the computations needed at each iteration of the algorithm.
\end{problem}

\end{document}
