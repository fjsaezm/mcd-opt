\documentclass[11pt,table]{article}

    \usepackage[table]{xcolor}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    

    %\usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    % Document title
    \title{Convex Optimization}

    
\usepackage{mdframed}


\newenvironment{problem}[2][Exercise]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}

% Define solution environment
\newenvironment{solution}
    {\textit{Solución}}
    {}
    


    \usepackage{fancyhdr} % Headers and footers
    \pagestyle{fancy} % All pages have headers and footers
    \fancyhead{}\renewcommand{\headrulewidth}{0pt} % Blank out the default header
    \fancyfoot[L]{} % Custom footer text
    \fancyfoot[C]{} % Custom footer text
    \fancyfoot[R]{\thepage} % Custom footer text
    \newcommand{\note}[1]{\marginpar{\scriptsize \textcolor{red}{#1}}} % Enables comments in red on margin
    
    


    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    
    \pagestyle{fancy}

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
% COMMANDS

\newcommand\norm[1]{\lVert#1\rVert}
\newcommand\R{\mathbb R}
    

\begin{document}
    
    %-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth}
\raggedright
\footnotesize
Francisco Javier Sáez Maldonado \hfill\\
José Antonio Álvarez Ocete \hfill\\
\end{minipage}
\begin{minipage}{0.4\textwidth}
\centering
\large
Ejercicios Programación Lineal\\
\normalsize
Optimización\\
\end{minipage}
\begin{minipage}{0.295\textwidth}
\raggedleft
\today\hfill\\
\end{minipage}
\medskip\hrule
\bigskip

%-------------------------------
%	CONTENTS
% -------------------------------

\begin{problem}{1}
  Show that if \( S \) is an open set, its complement \( S^c \) is closed, and viceversa.
\end{problem}

\(\boxed{\Rightarrow}\)\\

Let \(S\) be an open set. We know that \(S^c\) is a closed set if and only if for any sequence of elements \(\{x_n\} \subset S^c\) such that \(\{x_n\} \longrightarrow x\), then \(x \in S^c\). \\

Let us use this characterization to prove that \(S^c\) is closed. Let \(\{x_n\} \subset S^c\) such that \(\{x_n\} \longrightarrow x\). Suppose that \(x\) is not in \(S^c\). Then \(x\) must be in its complementary, \(S\). Since \(S\) is an open set we know that \(\exists \epsilon > 0\) such that \( B(x, \epsilon) \subset S\). \\

Since \(\{x_n\} \longrightarrow x\), for any \(\delta > 0 \exists n \in \mathbb{N}\) such that \( \parallel x - x_n \parallel < \delta\). In particular, for \(\delta = \epsilon\) there is a element of the succesion \(x_n\) in \( B(x, \epsilon) \subset S\), but \(\{x_n\} \subset S^c\). This contradictions implies that \(x\) is, in fact, in \(S^c\).

\(\boxed{\Leftarrow}\)\\

Let \(S\) be a set such that its complement $S^c$ is closed (that is, \(S^c = cl(S^c)\)). Let us show that \(S\) is open. \\

Let \(x \in S\). Then, \(x \notin S^c\), which implies \(x \notin cl(S^c)\). This implies that

\[
\exists \epsilon > 0 \text{ such that } B(x,\epsilon) \cap S^c = \emptyset \implies B(x,\epsilon) \subset S 
\]

Thus, showing that $S$ is open. \\

\begin{problem}{2}
  If \( S_1, S_2 \) are convex subsets, prove that the following are also convex sets:
  \begin{align*}
    S_{1} \cap S_{2} & = \{x \ : \ x \in S_{1} \text{ and } x \in S_{2}\}\\
    S_{1} + S_{2} & = \{x + x' \ : \ x \in S_{1}, x' \in S_{2}\}\\
    S_{1} - S_{2} & = \{x - x' \ : \ x \in S_{1}, x0 \in S_{2}\}
    \end{align*}

\end{problem}

\begin{itemize}
	\item Consider \(S_{1} \cap S_{2}  =  \{x \ : \ x \in S_{1} \text{ and } x \in S_{2}\}\). Let \(x,x' \in S_1 \cap S_2\) and consider the segment
	\[
		z \equiv \lambda x + (1-\lambda)x', lambda \in [0,1]
	\]
	
	Since \(x\) is in in \(S_1\), which is a convex set, \(z\) will also be in \(S_1\). Likewise, \(z\) will be in \(S_2\), and thus in the intersection \(S_{1} \cap S_{2}\). Since the segment is contained in the intersection, \(S_{1} \cap S_{2}\) is a convex subset.
	
	\item Consider \(S_{1} + S_{2}  =  \{x + z \ : \ x \in S_{1}, z \in S_{2}\}\). Now, let \(x+z \in S_1 + S_2\) and \(x'+z' \in S_1+S_2\). Also, consider the segment
	\begin{align*}
		\lambda(x+z) + (1-\lambda)(x'+z') & = \lambda x + \lambda z + (1-\lambda)x' + (1-\lambda z')\\
		& = \underbrace{\lambda x +(1-\lambda)x'}_{x'' \in S_1} + \underbrace{\lambda z + (1-\lambda)z'}_{z'' \in S_2}  
	\end{align*}
	Where the underbraces are true due to the convexity of \(S_1\) and \(S_2\), so we have
	\[
	x'' + z'' \in S_1 + S_2   
	\]
	so the segment is in the sum set, and thus, the set \(S_1 + S_2\) is convex.
	
	\item Consider \(S_{1} - S_{2}  =  \{x - z \ : \ x \in S_{1}, z \in S_{2}\}\). Now, let \(x-z \in S_1 - S_2\) and \(x'-z' \in S_1-S_2\). Also, consider the segment
	\begin{align*}
	\lambda(x-z) + (1-\lambda)(x'-z') & = \lambda x - \lambda z + (1-\lambda)x' - (1-\lambda z')\\
	& = \underbrace{\lambda x +(1-\lambda)x'}_{x'' \in S_1} - \underbrace{\left( \lambda z + (1-\lambda)z'\right)}_{z'' \in S_2}  
	\end{align*}
	Where the underbraces are true due to the convexity of \(S_1\) and \(S_2\), so we have
	\[
	x'' - z'' \in S_1 - S_2   
	\]
	so the segment is in the difference set, and thus, the set \(S_1 - S_2\) is convex.
\end{itemize}


\begin{problem}{3}
  If \( f: S \to \mathbb{R} \) is a convex function on the convex set \( S \), the set \( S_{min} = \{x : \ x \text{ is a minimum of f} \} \) is a convex set.
\end{problem}

Let \(y\) be the minimum of \(f(x)\): \(y = \min_x f(x)\). Then \( S_{min} = \{x \in S : \ f(x) = y \} \). We need need to show that for all \(x, x' \in S_{min}\) and for all \(\lambda \in [0,1]\):

\[
	z \equiv \lambda x + (1-\lambda) x' \in S_{min} \leftrightarrow f(z) = y
\]

Since \(S\) is convex, \(z\) is in \(S\), and since \(f\) is also convex:

\begin{align*}
	f(z) & = f\left(\lambda x + (1-\lambda) x'\right) \\
	& \leq \lambda f(x) + (1-\lambda) f(x') \\
	& = \lambda y + (1-\lambda) y \\
	& = y \\
\end{align*}

where we used that \(x, x'\in S_{min}\). But since \(y\) is the minimum of \(f\), the equality holds \(f(z) = y\). This means that \(z\) is in \(S_{min}\), therefore \(S_{min}\) is a convex set.

\begin{problem}{4}
  Given a quadratic form \( q(w) = w^TQw + bw + c \), with \( Q \) a symmetric \( d\times d \) matrix, \( w,b\) being \(d\times 1 \) vectors and \( c \) a real number, derive its gradient and Hessian.
\end{problem}

Let us start by unrolling the quadratic form expression:

\[
	q(w) = \sum_{i,j=1}^{d} Q_{ij} w_{i}w_{j} + \sum_{i=1}^{d}b_{i}w_{i} + c,
\]

and compute the partial derivative over the \(k-th\) component:

\[
	\frac{\partial q}{\partial w_k} (w) = \sum_{i=1}^{d} Q_{ik} w_{i} + \sum_{j=1}^{d} Q_{kj} w_{j} + b_{k}
\]

where \(k \in \{1, \ldots, d\}\). By using that \(Q\) is symmetric we obtain:

\begin{equation}
	\label{eq-4}
	\frac{\partial q}{\partial w_k} (w) = 2 \; \sum_{j=1}^{d} Q_{kj} w_{j} + b_{k}.
\end{equation}

That is, we are multiplying the \(k-th\) row of the \(Q\) matrix and multiplying it by \(w\). We can obtain gradient as a product of matrices using the previous expression:

\[
	\nabla q(w) = \begin{pmatrix}
		\frac{\partial q}{\partial w_1} (w) \\
		\vdots \\
		\frac{\partial q}{\partial w_d} (w)
	\end{pmatrix}
	= \begin{pmatrix}
		2 \; \sum_{j=1}^{d} Q_{1j} w_{j} + b_{1} \\
		\vdots \\
		2 \; \sum_{j=1}^{d} Q_{dj} w_{j} + b_{d}
	\end{pmatrix}
	= 2 \; \begin{pmatrix}
		\sum_{j=1}^{d} Q_{1j} w_{j} \\
		\vdots \\
		\sum_{j=1}^{d} Q_{dj} w_{j}
	\end{pmatrix} + \begin{pmatrix}
		b_{1} \\
		\vdots \\
		b_{d}
	\end{pmatrix}
	= 2 \ Qw + b
\]

In order to obtain the gradient take partial derivative over the \(l-th\) component in \ref{eq-4}:
 
\begin{align*}
	\frac{\partial^2 q}{\partial w_k \partial w_l} (w) & = \frac{\partial q}{\partial w_l} \left( \frac{\partial q}{\partial w_k} \right) (w) \\
	& = \frac{\partial q}{\partial w_l} \left( 2 \; \sum_{j=1}^{d} Q_{kj} w_{j} + b_{k} \right) (w) \\
	& = 2 \; Q_{kl}
\end{align*}

Hence, the Hessian matrix of \(q\) will have \(2 Q_{kl}\) in position \((k, l)\). That is:

\[
	\text{Hess} \ q(w) = 2Q
\]

\begin{problem}{5}
	If \( (p_1,\dots,p_n) \) is a probability distribution, prove that its entropy \( H(p_1, \dots, p_n) = - \sum_{i=1}^n p_i \log p_i\) is a concave function. Show also that its maximum is \( \log n \), attained when \( p_i = \frac{1}{n}\) for all $i$.
\end{problem}

\begin{problem}{6}
  We want to solve the following constrained restriction problem:
  \begin{align*}
    \min \quad &  x^{2} + 2y^{2} + 4xy \\
    \text{s.t} \quad &  x + y = 1 \\
    & x,y \geq 0.
  \end{align*}
  \begin{enumerate}
    \item Write its Lagrangian with \(\alpha,\beta\) the multipliers of the inequality constraints.
    \item Write the KKT conditions.
          \item Use them to solve the problem. For this consider separately the \((\alpha = \beta = 0)\), \((\alpha > 0, \beta = 0)\), \((\alpha = 0, \beta > 0)\), \((\alpha > 0, \beta > 0)\) cases.
\end{enumerate}
\end{problem}

\begin{problem}{7}
	We have worked out the dual problem for the soft SVC problem. Do the same for the simpler \textbf{hard} SVC problem
	\[
	\min_{w,b} \frac{1}{2} \norm{w}^2  
	\]
	subject to \(y^p\left(w \cdot x^p + b\right) \geq 1\). What are here the KKT conditions?
\end{problem}

Firstly, consider the Lagrangian for this problem
\begin{align*}
L(w,b,\lambda) & = \frac{1}{2} \norm{w}^2 - \sum \alpha_p \left[ y^p \left(w \cdot x^p +b\right) -1 \right]\\
& = \frac{1}{2} w \cdot w - w \sum \lambda_p y^p  x^p - b \sum \lambda_p y_p + \sum \lambda_p \\
& = w \left(\frac{1}{2}w -\sum \lambda_p y^p  x^p \right) b \sum \lambda_p y_p + \sum \lambda_p
\end{align*}

Then, we have to compute the gradient of the Lagrangian

\[
	\nabla_w L(w,b,\lambda) = w - \sum \lambda_p y^p x^p = 0 \implies w = \sum \lambda_p y^p x^p
\]
\[
	\frac{\partial L}{\partial b} = - \sum \lambda_p y^p = 0  
\]

Lastly, we have to use the optimalas that we have found in the expression of the Lagrangian:

\begin{align*}
	L(w,b,\alpha) &= \sum \alpha_p  - \frac{1}{2}\left(\sum_p \alpha_p y^p x^p\right)\left(\sum_q \alpha_q y^q x^q\right) \\
	& = \sum \alpha_p - \frac{1}{2} \sum_{p,q} \alpha_p y^p \alpha_q y^q x^p x^q = - \alpha
\end{align*}

Our optimization problem has turned today
\[
\max_{\alpha} \sum \alpha^p - \frac{1}{2}\sum \alpha_p \alpha_q y^p y^q x^p x^q  
\]
subject to \(\alpha^p, \sum \alpha_p y^p = 0\).

Let us state the KKT conditions for this problem.

\begin{problem}{8}
  A typical Linear Programming (LP) problem can be stated as the following constrained optimization problem:
  \begin{align*}
  \min_x c \cdot x \quad s.t. \quad x \geq 0, Ax \leq b
  \end{align*}
  with \(x \in \R^d\), \(A\) an \(m \times d\) matrix and \(b \in \R^m\). A tool often used in LP is to study the so called dual problem, which in this case is
  \begin{align*}
  \min_{z} b \cdot z \quad s.t. \quad z \geq 0, A^t z \leq -c
  \end{align*}
  with now \(z \in \R^m\). Apply our Lagrangian dual construction technique to show that this is indeed the dual formulation of the initial LP problem
\end{problem}

Firstly, we have to write the Lagrangian for this problem:
\[
L(x,\lambda,\mu) = c \cdot x -  \sum_{i=1}^d \lambda_i x_i  + \sum_{j=1}^m \mu_j(a_j \cdot x - b_j)  
\]
Hence, the gradient respect to \(x\) of the lagrangian is:
\[
\nabla_x L = c - \lambda + A^t \mu
\]
CHECK!!!\\
(this Lagrangian must have dimension \(d\)). 

\begin{problem}{11}
  If \( Q \) is a symmetric, positive definite \( d \times d \) matrix, show that \( f(x) = x^TQx, \ x \in \mathbb{R}^d \), is a convex function.
  \end{problem}

\begin{problem}{12}
  Let \( f:\mathbb{R}^d \to \mathbb{R} \)  be a function and assume that \( epi(f) \subset \mathbb{R}^d \times \mathbb{R} \)  is convex. Prove that then \( f \)  is convex.

\end{problem}



\begin{problem}{13}

  Let \( f : \mathbb{R}^d \to \mathbb{R} \) be a convex function. Prove that \( \operatorname{epi}(f) \) is a closed set and that \( (x, f(x)) \in \partial \operatorname{epi}(f) \)  .
\end{problem}

\begin{problem}{14}
  Prove that if \( f \)  is strictly convex, it has a unique global minimum.
\end{problem}


\begin{problem}{15}
  Let \( f,g: S \subset \mathbb{R}^d \to \mathbb{R} \)  be two convex functions on the convex set \( S \) . Prove that, as subsets, \( \partial(f+g)(x) \subset \partial f(x) + \partial g(x) \) for any $x\in S$.
  \end{problem}


\begin{problem}{16}
  Compute the proximal of \( f(x) = 0 \) and of \( g(x) = \frac{1}{2}\|x\|^2 \).
\end{problem}


\begin{problem}{17}
  Assume that \( f \)  is convex. Prove that for any \( \lambda > 0, \partial(\lambda f)(x) = \lambda \partial f(x) \) as subsets.
\end{problem}



\begin{problem}{19}Compute the proximals of the hinge \( f(x) = max\{0, -x\}  \) and the \( \epsilon \)-insensitive \( g(x)=max\{0, |x| - \epsilon\} \) loss functions.
  \end{problem}


\end{document}
